{
 "metadata": {
  "name": "",
  "signature": "sha256:003211e6d6057704e841b3629849d8b5fd582b1e86023dd3f79f2cdf1f4adfe9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas version:  0.13.1\n",
        "numpy version: 1.8.0\n",
        "sklearn version: 0.14.1\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys,os\n",
      "cwd=os.getcwd()\n",
      "path=cwd.split('/')\n",
      "home_dir='/'.join(path[:-2])\n",
      "print home_dir\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/UCSD_BigData\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir=home_dir+'/data/weather'\n",
      "!ls $data_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ALL.corrupted.csv           SAMPLE_TMAX.csv.old.gz      ghcnd-stations_buffered.txt\r\n",
        "ALL.corrupted.csv~          data-source.txt             ghcnd-version.txt\r\n",
        "ALL.head.csv                ghcnd-readme.txt\r\n",
        "SAMPLE_TMAX.csv             ghcnd-stations.txt\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls *.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ECatch.py              \u001b[31mStatistics.py\u001b[m\u001b[m          \u001b[31mmap-year-temp.py\u001b[m\u001b[m       mr_word_freq_count.py\r\n",
        "Stations_Statistics.py \u001b[31mcoding.py\u001b[m\u001b[m              mr_weather.py          \u001b[31mreduce-year-temp.py\u001b[m\u001b[m\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ECatch.py\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefor to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    print type(func)\n",
      "    f_name=func.__name__\n",
      "    @wraps(func)\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ECatch.py\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class C:\n",
      "    def increment_counter(self,a,b,n):\n",
      "        print 'increment counter(',a,b,n,')'\n",
      "    @ECatch\n",
      "    def Z(self,list):\n",
      "        print list\n",
      "        print sum(list)\n",
      "        return sum(list)\n",
      "CC=C()\n",
      "print CC.Z([1,'a',2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'function'>\n",
        "[1, 'a', 2]\n",
        "increment counter( C ERROR in Z 1 )\n",
        "None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Error:cannot perform reduce with flexible typeTraceback (most recent call last):\n",
        "  File \"<ipython-input-108-61ae942e428f>\", line 20, in inner\n",
        "    return func(self,*args,**kwargs)\n",
        "  File \"<ipython-input-122-2acc89746305>\", line 7, in Z\n",
        "    print sum(list)\n",
        "  File \"//anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.py\", line 1709, in sum\n",
        "    out=out, keepdims=keepdims)\n",
        "  File \"//anaconda/lib/python2.7/site-packages/numpy/core/_methods.py\", line 25, in _sum\n",
        "    out=out, keepdims=keepdims)\n",
        "TypeError: cannot perform reduce with flexible type\n",
        "Arguments were ([1, 'a', 2],), {}\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CC.__class__.__name__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 123,
       "text": [
        "'C'"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "type(f),f.im_class.__name__,f.im_func.__name__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 124,
       "text": [
        "(instancemethod, 'C', 'Z')"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile Stations_Statistics.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "collect the statistics for each station.\n",
      "\"\"\"\n",
      "import re,pickle,base64,zlib\n",
      "from sys import stderr\n",
      "import sys\n",
      "\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages') # a hack because anaconda made mrjob unreachable\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import *\n",
      "\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefor to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    f_name=func.__name__\n",
      "    @wraps(func)\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n",
      "\n",
      "\"\"\"\n",
      "Functions for encoding and decoding arbitrary object into ascii \n",
      "so that they can be passed through the hadoop streaming interface.\n",
      "\"\"\"\n",
      "\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def dumps(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "class MRWeather(MRJob):\n",
      "\n",
      "    @ECatch\n",
      "    def map_one(self,line):\n",
      "        return line.split(',')\n",
      "    \n",
      "    def mapper(self, _, line):\n",
      "        elements=self.map_one(line)\n",
      "        yield(elements[0],elements[1:])\n",
      "            \n",
      "    def check_integrity(self,meas,year,length):\n",
      "        year=int(year)\n",
      "        if year<1000 or year > 2014: return False\n",
      "        if meas=='': return False\n",
      "        if length != 367: return False\n",
      "        return True\n",
      "    \n",
      "    @ECatch\n",
      "    def reduce_one(self,S,vector):\n",
      "        meas=vector[0]\n",
      "        year=vector[1]\n",
      "        length=len(vector)\n",
      "        number_defined=sum([e!='' for e in vector[2:]])\n",
      "        assert self.check_integrity(meas,year,length)==True\n",
      "        S[(meas,int(year))]=number_defined\n",
      "        \n",
      "    def reducer(self, station, vectors):\n",
      "        S={}\n",
      "        for vector in vectors:\n",
      "            self.reduce_one(S,vector)\n",
      "        yield(station,dumps(S))\n",
      "                              \n",
      "if __name__ == '__main__':\n",
      "    MRWeather.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting Stations_Statistics.py\n"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running job inline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -1 $data_dir/ALL.head.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ASN00054128,DAPR,1969,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\r\n"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: Stations_Statistics.py [options] [input files]\r\n",
        "\r\n",
        "Options:\r\n",
        "  --help                show this message and exit\r\n",
        "  --help-emr            show EMR-related options\r\n",
        "  --help-hadoop         show Hadoop-related options\r\n",
        "  --help-runner         show runner-related options\r\n",
        "\r\n",
        "  Running specific parts of the job:\r\n",
        "    --mapper            run a mapper\r\n",
        "    --combiner          run a combiner\r\n",
        "    --reducer           run a reducer\r\n",
        "    --step-num=STEP_NUM\r\n",
        "                        which step to execute (default is 0)\r\n",
        "    --steps             print the mappers, combiners, and reducers that this job defines\r\n",
        "\r\n",
        "  Protocols:\r\n",
        "    --strict-protocols  If something violates an input/output protocol then raise an exception\r\n"
       ]
      }
     ],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py $data_dir/ALL.corrupted.csv > StationStatistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /Users/yoavfreund/.mrjob.conf\r\n",
        "creating tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650\r\n",
        "writing to /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  MRWeather:\r\n",
        "    total in map_one: 999\r\n",
        "writing to /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650/step-0-mapper-sorted\r\n",
        "> sort /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650/step-0-reducer_part-00000\r\n",
        "Error:invalid literal for int() with base 10: '19AA2'Traceback (most recent call last):\r\n",
        "  File \"Stations_Statistics.py\", line 32, in inner\r\n",
        "    return func(self,*args,**kwargs)\r\n",
        "  File \"Stations_Statistics.py\", line 78, in reduce_one\r\n",
        "    assert self.check_integrity(meas,year,length)==True\r\n",
        "  File \"Stations_Statistics.py\", line 66, in check_integrity\r\n",
        "    year=int(year)\r\n",
        "ValueError: invalid literal for int() with base 10: '19AA2'\r\n",
        "Arguments were ({('DAPR', 1976): 1, ('DAPR', 1969): 1, ('DAPR', 1981): 2, ('DAPR', 1971): 2, ('DAPR', 1979): 1}, ['DAPR', '19AA2', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', '2', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', '2', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'']), {}\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  MRWeather:\r\n",
        "    errors in reduce_one: 1\r\n",
        "    total in map_one: 999\r\n",
        "    total in reduce_one: 999\r\n",
        "Moving /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650/step-0-reducer_part-00000 -> /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650/output/part-00000\r\n",
        "Streaming final output from /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650/output\r\n",
        "removing tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082931.044650\r\n"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py -r emr --emr-job-flow-id $job_flow_id $data_dir/ALL.corrupted.csv > StationStatistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /Users/yoavfreund/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082947.173614\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://yoav.hadoop/scratch/Stations_Statistics.yoavfreund.20140517.082947.173614/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-262J0JTFJIRLO\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.6s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.082947.173614: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 61.4s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.082947.173614: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 73.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  File Input Format Counters :\r\n",
        "    Bytes Read: 1052616\r\n",
        "  File Output Format Counters :\r\n",
        "    Bytes Written: 11191\r\n",
        "  FileSystemCounters:\r\n",
        "    FILE_BYTES_READ: 401711\r\n",
        "    FILE_BYTES_WRITTEN: 2687767\r\n",
        "    HDFS_BYTES_READ: 9300\r\n",
        "    S3_BYTES_READ: 1052616\r\n",
        "    S3_BYTES_WRITTEN: 11191\r\n",
        "  Job Counters :\r\n",
        "    Launched map tasks: 60\r\n",
        "    Launched reduce tasks: 8\r\n",
        "    Rack-local map tasks: 60\r\n",
        "    SLOTS_MILLIS_MAPS: 460578\r\n",
        "    SLOTS_MILLIS_REDUCES: 115910\r\n",
        "    Total time spent by all maps waiting after reserving slots (ms): 0\r\n",
        "    Total time spent by all reduces waiting after reserving slots (ms): 0\r\n",
        "  MRWeather:\r\n",
        "    errors in reduce_one: 1\r\n",
        "    total in map_one: 999\r\n",
        "    total in reduce_one: 999\r\n",
        "  Map-Reduce Framework:\r\n",
        "    CPU time spent (ms): 57740\r\n",
        "    Combine input records: 0\r\n",
        "    Combine output records: 0\r\n",
        "    Map input bytes: 858961\r\n",
        "    Map input records: 999\r\n",
        "    Map output bytes: 1963855\r\n",
        "    Map output materialized bytes: 408693\r\n",
        "    Map output records: 999\r\n",
        "    Physical memory (bytes) snapshot: 26726682624\r\n",
        "    Reduce input groups: 11\r\n",
        "    Reduce input records: 999\r\n",
        "    Reduce output records: 11\r\n",
        "    Reduce shuffle bytes: 408693\r\n",
        "    SPLIT_RAW_BYTES: 9300\r\n",
        "    Spilled Records: 1998\r\n",
        "    Total committed heap usage (bytes): 27200585728\r\n",
        "    Virtual memory (bytes) snapshot: 126696419328\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Streaming final output from s3://yoav.hadoop/scratch/Stations_Statistics.yoavfreund.20140517.082947.173614/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.082947.173614\r\n",
        "Removing all files in s3://yoav.hadoop/scratch/Stations_Statistics.yoavfreund.20140517.082947.173614/\r\n"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## managing AWS Credentials ##\n",
      "The full details on how to get the credentials set up is given here: https://docs.google.com/document/d/1xDUy4ZI2yr1eCCRQ4ynWHsctbEb7ySHrSynBu0bxupU/edit?usp=sharing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!echo $EC2_VAULT"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/yoavfreund/BigData/Vault\r\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "Creds_file=os.environ['EC2_VAULT']+'/Creds.pkl'\n",
      "Creds= pickle.load(open(Creds_file,'rb'))\n",
      "print Creds.keys()\n",
      "print Creds['mrjob'].keys()\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "print ID,key_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['launcher', 'admin', 'mrjob']\n",
        "['key_id', 'secret_key', 's3_logs', 'ID', 's3_scratch']\n",
        "Yoav_Student AKIAJ3SIH7HZ53WL4YMA\n"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Checking for an available job flow###\n",
      "Before submitting your job for execution you need to find out which job flows are active and waiting. THe following cell will do that for you. If there is a waiting cluster, it will put it's ID into the variable `job_flow_id`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "job_flow_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<boto.emr.emrobject.JobFlow object at 0x102af1f10> no_script.yoavfreund.20140516.040032.370095 j-262J0JTFJIRLO WAITING\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 168,
       "text": [
        "u'j-262J0JTFJIRLO'"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setting up the mrjob configuration file ###\n",
      "The last step before submitting the job is to insert the credentials into the file `/home/ubuntu/UCSD_BigData/utils/mrjob.conf.template` and put the result in the default location for the mrjob configuration file which is: `/home/ubuntu/.mrjob.conf`\n",
      "this is done by the following line. If the return value is `True` you are good to go. If it is `False` something went wrong and you should get an error message.\n",
      "\n",
      "The template file should work as is. However, feel free to change and add fields to this configuration file to make it your own.\n",
      "\n",
      "This step needs to be done just one time. Redone only when starting a new EC2 instance or if the credentials changed.\n",
      "It is better to do it in an interactive shell, rather than in a notebook. Here it is done in the notebook for demonstration purpose."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd /home/ubuntu/UCSD_BigData/utils/\n",
      "!python Make.mrjob.conf.py\n",
      "%cd ../notebooks/weather.mapreduce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Errno 2] No such file or directory: '/home/ubuntu/UCSD_BigData/utils/'\n",
        "/Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/UCSD_BigData/notebooks/weather.mapreduce\n",
        "python: can't open file 'Make.mrjob.conf.py': [Errno 2] No such file or directory\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Errno 2] No such file or directory: '../notebooks/weather.mapreduce/'\n",
        "/Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/UCSD_BigData/notebooks/weather.mapreduce\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assuming the steps up to here were all successful, you should be ready to launch your mrjob job. There is no need to change anything\n",
      "in the following line."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py -r emr --emr-job-flow-id  $job_flow_id  hdfs:/weather/weather.csv > Statistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /Users/yoavfreund/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.083253.972531\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://yoav.hadoop/scratch/Stations_Statistics.yoavfreund.20140517.083253.972531/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-262J0JTFJIRLO\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.6s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 61.3s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 92.0s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 122.7s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 153.3s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 184.2s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 214.8s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 245.4s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 276.1s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 306.8s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 337.4s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 368.1s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 398.7s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 429.4s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 460.1s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 490.8s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 521.7s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 552.8s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 583.4s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 614.0s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 644.7s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 675.4s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 706.0s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 736.7s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 767.5s ago, status RUNNING: Running step (Stations_Statistics.yoavfreund.20140517.083253.972531: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 739.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  File Input Format Counters :\r\n",
        "    Bytes Read: 7670788923\r\n",
        "  File Output Format Counters :\r\n",
        "    Bytes Written: 107043220\r\n",
        "  FileSystemCounters:\r\n",
        "    FILE_BYTES_READ: 6942522148\r\n",
        "    FILE_BYTES_WRITTEN: 10374515566\r\n",
        "    HDFS_BYTES_READ: 7670794863\r\n",
        "    S3_BYTES_WRITTEN: 107043220\r\n",
        "  Job Counters :\r\n",
        "    Data-local map tasks: 58\r\n",
        "    Launched map tasks: 62\r\n",
        "    Launched reduce tasks: 10\r\n",
        "    Rack-local map tasks: 4\r\n",
        "    SLOTS_MILLIS_MAPS: 3879481\r\n",
        "    SLOTS_MILLIS_REDUCES: 3177643\r\n",
        "    Total time spent by all maps waiting after reserving slots (ms): 0\r\n",
        "    Total time spent by all reduces waiting after reserving slots (ms): 0\r\n",
        "  MRWeather:\r\n",
        "    errors in reduce_one: 1\r\n",
        "    total in map_one: 9358395\r\n",
        "    total in reduce_one: 9358395\r\n",
        "  Map-Reduce Framework:\r\n",
        "    CPU time spent (ms): 4688660\r\n",
        "    Combine input records: 0\r\n",
        "    Combine output records: 0\r\n",
        "    Map input bytes: 7668890105\r\n",
        "    Map input records: 9358395\r\n",
        "    Map output bytes: 18019274975\r\n",
        "    Map output materialized bytes: 3457612075\r\n",
        "    Map output records: 9358395\r\n",
        "    Physical memory (bytes) snapshot: 36359675904\r\n",
        "    Reduce input groups: 85284\r\n",
        "    Reduce input records: 9358395\r\n",
        "    Reduce output records: 85284\r\n",
        "    Reduce shuffle bytes: 3457612075\r\n",
        "    SPLIT_RAW_BYTES: 5940\r\n",
        "    Spilled Records: 28075185\r\n",
        "    Total committed heap usage (bytes): 33475788800\r\n",
        "    Virtual memory (bytes) snapshot: 127089926144\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Streaming final output from s3://yoav.hadoop/scratch/Stations_Statistics.yoavfreund.20140517.083253.972531/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing tmp directory /var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/Stations_Statistics.yoavfreund.20140517.083253.972531\r\n",
        "Removing all files in s3://yoav.hadoop/scratch/Stations_Statistics.yoavfreund.20140517.083253.972531/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Traceback (most recent call last):\r\n",
        "  File \"Stations_Statistics.py\", line 88, in <module>\r\n",
        "    MRWeather.run()\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 494, in run\r\n",
        "    mr_job.execute()\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 512, in execute\r\n",
        "    super(MRJob, self).execute()\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 147, in execute\r\n",
        "    self.run_job()\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 211, in run_job\r\n",
        "    for line in runner.stream_output():\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/runner.py\", line 486, in stream_output\r\n",
        "    for line in self._cat_file(filename):\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/fs/composite.py\", line 71, in _cat_file\r\n",
        "    for line in self._do_action('_cat_file', path):\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/util.py\", line 444, in read_file\r\n",
        "    for line in lines:\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/mrjob/util.py\", line 73, in buffer_iterator_to_line_iterator\r\n",
        "    for chunk in iterator:\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/boto/s3/key.py\", line 368, in next\r\n",
        "    data = self.resp.read(self.BufferSize)\r\n",
        "  File \"//anaconda/lib/python2.7/site-packages/boto/connection.py\", line 416, in read\r\n",
        "    return httplib.HTTPResponse.read(self, amt)\r\n",
        "  File \"//anaconda/lib/python2.7/httplib.py\", line 567, in read\r\n",
        "    s = self.fp.read(amt)\r\n",
        "  File \"//anaconda/lib/python2.7/socket.py\", line 380, in read\r\n",
        "    data = self._sock.recv(left)\r\n",
        "  File \"//anaconda/lib/python2.7/ssl.py\", line 241, in recv\r\n",
        "    return self.read(buflen)\r\n",
        "  File \"//anaconda/lib/python2.7/ssl.py\", line 160, in read\r\n",
        "    return self._sslobj.read(len)\r\n",
        "ssl.SSLError: The read operation timed out\r\n"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    }
   ],
   "metadata": {}
  }
 ]
}